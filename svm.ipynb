{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus = pd.read_csv(\"corpus.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step - a : Remove blank rows if any.\n",
    "Corpus['text'].dropna(inplace=True)# Step - b : Change all the text to lower case. This is required as python interprets 'dog' and 'DOG' differently\n",
    "Corpus['text'] = [entry.lower() for entry in Corpus['text']]# Step - c : Tokenization : In this each entry in the corpus will be broken into set of words\n",
    "Corpus['text']= [word_tokenize(entry) for entry in Corpus['text']]# Step - d : Remove Stop words, Non-Numeric and perfom Word Stemming/Lemmenting.# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "for index,entry in enumerate(Corpus['text']):\n",
    "    # Declaring Empty List to store the words that follow the rules for this step\n",
    "    Final_words = []\n",
    "    # Initializing WordNetLemmatizer()\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    for word, tag in pos_tag(entry):\n",
    "        # Below condition is to check for Stop words and consider only alphabets\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "    Corpus.loc[index,'text_final'] = str(Final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(Corpus['text_final'],Corpus['label'],test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Encoder = LabelEncoder()\n",
    "Train_Y = Encoder.fit_transform(Train_Y)\n",
    "Test_Y = Encoder.fit_transform(Test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "Tfidf_vect.fit(Corpus['text_final'])\n",
    "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(Test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stun': 322, 'even': 96, 'sound': 308, 'track': 346, 'beautiful': 21, 'paint': 237, 'senery': 295, 'mind': 208, 'well': 369, 'would': 383, 'recomend': 272, 'people': 241, 'hate': 152, 'video': 364, 'game': 131, 'music': 217, 'play': 248, 'chrono': 42, 'cross': 62, 'ever': 98, 'best': 26, 'back': 19, 'away': 18, 'crude': 63, 'keyboarding': 181, 'take': 329, 'fresh': 125, 'step': 319, 'grate': 144, 'guitar': 149, 'soulful': 307, 'orchestra': 230, 'impress': 167, 'anyone': 11, 'care': 39, 'listen': 193, 'soundtrack': 309, 'anything': 12, 'read': 267, 'lot': 198, 'review': 281, 'say': 288, 'figure': 114, 'write': 384, 'disagree': 72, 'bit': 29, 'opinino': 228, 'yasunori': 387, 'mitsuda': 211, 'ultimate': 355, 'masterpiece': 204, 'timeless': 340, 'year': 388, 'beauty': 23, 'simply': 300, 'refuse': 274, 'price': 255, 'tag': 328, 'pretty': 254, 'stagger': 315, 'must': 218, 'go': 139, 'buy': 35, 'cd': 40, 'much': 216, 'money': 213, 'one': 227, 'feel': 111, 'worth': 382, 'every': 99, 'penny': 240, 'amazing': 6, 'favorite': 110, 'time': 339, 'hand': 151, 'intense': 174, 'sadness': 286, 'prisoner': 257, 'fate': 109, 'mean': 206, 'hope': 160, 'distant': 76, 'promise': 260, 'girl': 135, 'steal': 318, 'star': 316, 'important': 166, 'inspiration': 172, 'personally': 244, 'throughout': 338, 'teen': 330, 'high': 157, 'energy': 89, 'like': 192, 'dreamwatch': 83, 'chronomantique': 43, 'indefinably': 169, 'remeniscent': 278, 'trigger': 349, 'absolutely': 1, 'superb': 323, 'probably': 258, 'composer': 50, 'work': 380, 'hear': 154, 'xenogears': 386, 'ca': 37, 'sure': 324, 'never': 220, 'twice': 352, 'wish': 376, 'could': 54, 'give': 137, 'excellent': 102, 'truly': 351, 'enjoy': 91, 'relaxing': 275, 'disk': 75, 'scar': 289, 'life': 190, 'death': 66, 'forest': 121, 'illusion': 164, 'fortress': 123, 'ancient': 8, 'dragon': 81, 'lose': 197, 'fragment': 124, 'drown': 84, 'two': 353, 'draggons': 80, 'galdorb': 129, 'home': 159, 'gale': 130, 'girlfriend': 136, 'zelbessdisk': 390, 'three': 337, 'garden': 132, 'god': 140, 'chronopolis': 44, 'jellyfish': 177, 'sea': 291, 'burn': 34, 'orphange': 231, 'prayer': 251, 'tower': 345, 'radical': 265, 'dreamer': 82, 'unstealable': 360, 'bring': 33, 'remember': 277, 'pull': 262, 'jaw': 175, 'floor': 118, 'know': 185, 'divine': 77, 'single': 302, 'song': 305, 'tell': 331, 'story': 321, 'good': 142, 'great': 145, 'without': 377, 'doubt': 79, 'magical': 202, 'wind': 375, 'unstolen': 361, 'jewel': 178, 'translation': 347, 'varies': 363, 'perfect': 242, 'ask': 14, 'pour': 250, 'heart': 156, 'paper': 238, 'absolute': 0, 'quite': 264, 'actually': 3, 'least': 187, 'heard': 155, 'whether': 370, 'aware': 17, 'contribute': 52, 'greatly': 146, 'mood': 214, 'minute': 209, 'whole': 372, 'exact': 101, 'count': 55, 'impressively': 168, 'remarkable': 276, 'assure': 15, 'forget': 122, 'everything': 100, 'listener': 194, 'energetic': 87, 'dance': 64, 'tokage': 342, 'termina': 334, 'slow': 303, 'haunting': 153, 'purely': 263, 'beautifully': 22, 'compose': 49, 'fantastic': 107, 'vocal': 366, 'videogame': 365, 'soundtracks': 310, 'surely': 325, 'buyer': 36, 'beware': 27, 'book': 30, 'want': 367, 'paragraph': 239, 'haddon': 150, 'family': 106, 'friend': 126, 'perhaps': 243, 'imagine': 165, 'thing': 335, 'spend': 314, 'evening': 97, 'hysteric': 163, 'piece': 246, 'another': 10, 'definitely': 68, 'bad': 20, 'enough': 92, 'enter': 93, 'kind': 184, 'contest': 51, 'believe': 25, 'amazon': 7, 'sell': 293, 'maybe': 205, 'offer': 226, 'grade': 143, 'term': 333, 'kill': 183, 'mockingbird': 212, 'anyway': 13, 'unless': 359, 'send': 294, 'someone': 304, 'joke': 179, 'far': 108, 'glorious': 138, 'love': 200, 'whisper': 371, 'wicked': 373, 'saint': 287, 'pleasantly': 249, 'surprise': 327, 'change': 41, 'normaly': 221, 'romance': 283, 'novel': 222, 'world': 381, 'rave': 266, 'brilliant': 32, 'true': 350, 'wonderful': 379, 'typical': 354, 'crime': 60, 'becuase': 24, 'miss': 210, 'warm': 368, 'five': 117, 'finish': 116, 'fell': 112, 'caracters': 38, 'expect': 104, 'average': 16, 'instead': 173, 'find': 115, 'think': 336, 'predict': 253, 'outcome': 232, 'shock': 299, 'writting': 385, 'descriptive': 70, 'break': 31, 'julia': 180, 'felt': 113, 'reader': 268, 'lover': 201, 'let': 189, 'cover': 58, 'fool': 119, 'spectacular': 313, 'disappointed': 73, 'romanian': 285, 'opinion': 229, 'bias': 28, 'angle': 9, 'europe': 95, 'clean': 46, 'proper': 261, 'fail': 105, 'shed': 298, 'light': 191, 'rest': 279, 'understand': 357, 'tourist': 344, 'guide': 148, 'tour': 343, 'country': 56, 'however': 161, 'use': 362, 'reference': 273, 'see': 292, 'travel': 348, 'outline': 233, 'exclude': 103, 'romania': 284, 'precision': 252, 'detail': 71, 'dk': 78, 'series': 297, 'leave': 188, 'yet': 389, 'disappointment': 74, 'lonely': 195, 'planet': 247, 'full': 128, 'picture': 245, 'info': 171, 'need': 219, 'also': 5, 'end': 86, 'indiv': 170, 'city': 45, 'scratch': 290, 'surface': 326, 'receive': 271, 'defective': 67, 'move': 215, 'germany': 133, 'get': 134, 'overview': 235, 'unfortunately': 358, 'page': 236, 'greece': 147, 'english': 90, 'look': 196, 'spanish': 312, 'sort': 306, 'printing': 256, 'problem': 259, 'highlight': 158, 'label': 186, 'memphis': 207, 'tn': 341, 'come': 47, 'reatards': 270, 'front': 127, 'course': 57, 'wild': 374, 'kid': 182, 'jay': 176, 'reatard': 269, 'start': 317, 'eric': 94, 'oblivians': 223, 'goner': 141, 'tender': 332, 'age': 4, 'still': 320, 'rock': 282, 'offend': 225, 'sensibility': 296, 'community': 48, 'overdriven': 234, 'loud': 199, 'crackle': 59, 'underlay': 356, 'southern': 311, 'croon': 61, 'howl': 162, 'energize': 88, 'either': 85, 'depend': 69, 'foot': 120, 'od': 224, 'copper': 53, 'wo': 378, 'accept': 2, 'return': 280, 'since': 301, 'make': 203, 'deal': 65}\n"
     ]
    }
   ],
   "source": [
    "print(Tfidf_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 385)\t0.15181783245925792\n",
      "  (0, 373)\t0.13182822297400662\n",
      "  (0, 371)\t0.13182822297400662\n",
      "  (0, 339)\t0.08347290620977138\n",
      "  (0, 336)\t0.15181783245925792\n",
      "  (0, 316)\t0.08347290620977138\n",
      "  (0, 313)\t0.15181783245925792\n",
      "  (0, 299)\t0.15181783245925792\n",
      "  (0, 287)\t0.13182822297400662\n",
      "  (0, 283)\t0.26365644594801324\n",
      "  (0, 268)\t0.15181783245925792\n",
      "  (0, 267)\t0.29296727954779\n",
      "  (0, 253)\t0.15181783245925792\n",
      "  (0, 232)\t0.15181783245925792\n",
      "  (0, 227)\t0.08347290620977138\n",
      "  (0, 222)\t0.13182822297400662\n",
      "  (0, 218)\t0.11764536933451467\n",
      "  (0, 201)\t0.15181783245925792\n",
      "  (0, 200)\t0.13182822297400662\n",
      "  (0, 189)\t0.15181783245925792\n",
      "  (0, 180)\t0.15181783245925792\n",
      "  (0, 173)\t0.30363566491851585\n",
      "  (0, 156)\t0.11764536933451467\n",
      "  (0, 119)\t0.15181783245925792\n",
      "  (0, 117)\t0.15181783245925792\n",
      "  :\t:\n",
      "  (9, 279)\t0.1256205421490419\n",
      "  (9, 273)\t0.1446688576328972\n",
      "  (9, 261)\t0.1446688576328972\n",
      "  (9, 252)\t0.1446688576328972\n",
      "  (9, 233)\t0.1446688576328972\n",
      "  (9, 229)\t0.1446688576328972\n",
      "  (9, 220)\t0.11210554723202179\n",
      "  (9, 191)\t0.1446688576328972\n",
      "  (9, 188)\t0.1446688576328972\n",
      "  (9, 161)\t0.1446688576328972\n",
      "  (9, 148)\t0.2512410842980838\n",
      "  (9, 122)\t0.1256205421490419\n",
      "  (9, 105)\t0.1446688576328972\n",
      "  (9, 103)\t0.1446688576328972\n",
      "  (9, 95)\t0.2512410842980838\n",
      "  (9, 78)\t0.1446688576328972\n",
      "  (9, 74)\t0.1446688576328972\n",
      "  (9, 73)\t0.2893377152657944\n",
      "  (9, 71)\t0.1446688576328972\n",
      "  (9, 56)\t0.1256205421490419\n",
      "  (9, 46)\t0.1446688576328972\n",
      "  (9, 30)\t0.1716307966027709\n",
      "  (9, 28)\t0.1446688576328972\n",
      "  (9, 11)\t0.11210554723202179\n",
      "  (9, 9)\t0.1446688576328972\n"
     ]
    }
   ],
   "source": [
    "print(Train_X_Tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Score ->  60.0\n"
     ]
    }
   ],
   "source": [
    "# fit the training dataset on the NB classifier\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
    "predictions_NB = Naive.predict(Test_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Test_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  60.0\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
